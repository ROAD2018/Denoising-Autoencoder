import torch,random
from model_building.lstm_encoder_decoder import Seq2Seq,Encoder,Decoder
from ensemble import path_lstm
from model_building import n_features,seq_len,batch_size,hidden_dim,device,SOS_token


def get_model_lstm():
    """
    :return: model loaded from disk
    """
    checkpoint = torch.load(path_lstm)
    encoder = Encoder(n_features=n_features,seq_len=seq_len,hidden_dim=hidden_dim,batch_size=batch_size).to(device)
    decoder = Decoder(n_features=n_features,seq_len=seq_len,hidden_dim=hidden_dim,batch_size=batch_size).to(device)
    model  = Seq2Seq(encoder,decoder)
    model.load_state_dict(checkpoint['model_state_dict'])
    return model


def predict(input_tensor, model):
    """
    :param input_tensor: input data to predict
    :param model: lstm model used to predict
    :return: output generated by model
    """
    model.eval()
    with torch.no_grad():
        input_tensor = input_tensor.view(-1, batch_size)
        encoder_hidden, encoder_cell = model.encoder(input_tensor)
        decoder_ops = torch.zeros(seq_len, batch_size, 1, device=device)
        decoder_input = torch.tensor([[SOS_token]] * batch_size, device=device)
        decoder_hidden = encoder_hidden
        decoder_cell = encoder_cell
        for di in range(seq_len):
            decoder_output, decoder_hidden, decoder_cell = model.decoder(
                decoder_input, decoder_hidden, decoder_cell)
            decoder_ops[di] = decoder_output.view(batch_size, 1)
            decoder_input = decoder_output.view(batch_size,1).squeeze().detach()
        return decoder_ops
